---
title: "AI Models & Data Privacy"
section: "FAQ"
description: "Exactly how Char handles your data: what stays local, what gets sent externally, and to whom."
---

This page documents every way Char stores and processes your data. Nothing is hidden. Where possible, we include code snippets from the actual codebase so you can verify each claim yourself.

## Local Data Storage

All core data is stored locally on your device by default. Nothing leaves your machine unless you explicitly enable a cloud feature.

### Where Files Live

Char uses two base directories. See [Data](/docs/guides/data) for the full directory layout.

**Global base** (shared across stable and nightly builds):
- `db.sqlite` — SQLite database (sessions, contacts, calendar events, tags)
- `models/stt/` — downloaded speech-to-text model files (Whisper GGUF, Argmax tarballs)
- `store.json` — app state (onboarding status, pinned tabs, recently opened sessions, dismissed toasts, analytics preference, auth tokens)
- `hyprnote.json` — vault configuration (custom vault path if set)
- `search/` — Tantivy full-text search index

On macOS, this is typically `~/Library/Application Support/hyprnote/`. On Linux, `~/.local/share/hyprnote/`.

The `store.json` keys are defined by this enum — nothing else is persisted through the Tauri store:

```rust
// apps/desktop/src-tauri/src/store.rs
pub enum StoreKey {
    OnboardingNeeded2,
    DismissedToasts,
    OnboardingLocal,
    TinybaseValues,
    PinnedTabs,
    RecentlyOpenedSessions,
}
```

**Vault base** (customizable, defaults to the global base):
- `sessions/` — one subdirectory per session containing recorded audio, transcripts, notes, and attachments
- `humans/` — contact and participant data
- `organizations/` — organization data
- `chats/` — chat conversation data
- `prompts/` — custom prompt templates
- `settings.json` — your app settings

**Application logs** are stored in the system app log directory as rotating files (`app.log`, `app.log.1`, etc.).

### How Session Data Is Stored on Disk

Each session gets its own subdirectory. Here is how Char loads session content from disk — this shows exactly what files exist per session:

```rust
// plugins/fs-sync/src/session_content.rs
pub fn load_session_content(
    session_id: &str,
    session_dir: &std::path::Path,
) -> SessionContentData {
    let mut content = SessionContentData {
        session_id: session_id.to_string(),
        meta: None,                  // from _meta.json
        raw_memo_tiptap_json: None,  // from memo.md
        transcript: None,            // from transcript.json
        notes: vec![],               // from *.md enhanced notes
    };

    // ...reads every file in the session directory:
    // - _meta.json -> SessionMetaData (title, created date, participants)
    // - transcript.json -> TranscriptData (words with timestamps)
    // - *.md -> AI-generated enhanced notes (parsed from Markdown
    //          with YAML frontmatter)
}
```

Transcript data uses this structure — word-level timestamps, speaker channels, and optional speaker hints:

```rust
// plugins/fs-sync/src/types.rs
pub struct TranscriptWord {
    pub id: Option<String>,
    pub text: String,
    pub start_ms: i64,
    pub end_ms: i64,
    pub channel: i64,
}

pub struct TranscriptEntry {
    pub id: String,
    pub user_id: Option<String>,
    pub created_at: Option<String>,
    pub session_id: String,
    pub started_at: Option<i64>,
    pub ended_at: Option<i64>,
    pub words: Vec<TranscriptWord>,
    pub speaker_hints: Vec<TranscriptSpeakerHint>,
}

pub struct TranscriptData {
    pub transcripts: Vec<TranscriptEntry>,
}
```

### Is My Data Encrypted at Rest?

Char stores data as plain SQLite, JSON, and Markdown files on disk. Char does not add its own encryption layer. Your data is protected by your operating system's file permissions and any full-disk encryption you have enabled (such as FileVault on macOS or LUKS on Linux).

## How Audio Recording Works

When you start a recording session, Char spawns three actors in parallel:

1. **SourceActor** — captures audio from your microphone and system speaker
2. **RecorderActor** — writes audio samples to local WAV files
3. **ListenerActor** — streams audio to your configured STT provider (cloud or local)

Here is the session supervisor that orchestrates these actors:

```rust
// plugins/listener/src/actors/session/supervisor.rs
async fn pre_start(
    &self,
    myself: ActorRef<Self::Msg>,
    ctx: Self::Arguments,
) -> Result<Self::State, ActorProcessingErr> {
    // 1. Always spawn the audio source
    let (source_ref, _) = Actor::spawn_linked(
        Some(SourceActor::name()),
        SourceActor,
        SourceArgs {
            mic_device: None,
            onboarding: ctx.params.onboarding,
            app: ctx.app.clone(),
            session_id: ctx.params.session_id.clone(),
        },
        myself.get_cell(),
    )
    .await?;

    // 2. Only spawn the recorder if recording is enabled
    let recorder_cell = if ctx.params.record_enabled {
        let (recorder_ref, _) = Actor::spawn_linked(
            Some(RecorderActor::name()),
            RecorderActor,
            RecArgs {
                app_dir: ctx.app_dir.clone(),
                session_id: ctx.params.session_id.clone(),
            },
            myself.get_cell(),
        )
        .await?;
        Some(recorder_ref.get_cell())
    } else {
        None
    };

    // ...
}
```

Audio is written to WAV files on your local disk. Here is the recorder handling incoming audio samples:

```rust
// plugins/listener/src/actors/recorder.rs
async fn handle(
    &self,
    _myself: ActorRef<Self::Msg>,
    msg: Self::Msg,
    st: &mut Self::State,
) -> Result<(), ActorProcessingErr> {
    match msg {
        RecMsg::AudioSingle(samples) => {
            if let Some(ref mut writer) = st.writer {
                if st.is_stereo {
                    write_mono_as_stereo(writer, &samples)?;
                } else {
                    write_mono_samples(writer, &samples)?;
                }
            }
        }
        RecMsg::AudioDual(mic, spk) => {
            if let Some(ref mut writer) = st.writer {
                if st.is_stereo {
                    write_interleaved_stereo(writer, &mic, &spk)?;
                } else {
                    let mixed = mix_audio_f32(&mic, &spk);
                    write_mono_samples(writer, &mixed)?;
                }
            }
            if let Some(ref mut writer_mic) = st.writer_mic {
                write_mono_samples(writer_mic, &mic)?;
            }
            if let Some(ref mut writer_spk) = st.writer_spk {
                write_mono_samples(writer_spk, &spk)?;
            }
        }
    }
    flush_if_due(st)?;
    Ok(())
}
```

Audio files are stored at `{vault}/sessions/{session_id}/audio.wav` — they never leave your device unless you explicitly use cloud transcription.

## What Data Leaves Your Device

The following sections document every case where Char sends data to an external server.

### Analytics (Opt-Out Available)

Char collects anonymous usage analytics by default to help improve the product. **You can disable this entirely in Settings.**

Here is the opt-out check — when disabled, the function returns immediately without sending anything:

```rust
// plugins/analytics/src/ext.rs
pub async fn event(
    &self,
    mut payload: hypr_analytics::AnalyticsPayload,
) -> Result<(), crate::Error> {
    Self::enrich_payload(self.manager, &mut payload);

    if self.is_disabled().unwrap_or(true) {
        return Ok(());
    }

    let machine_id = hypr_host::fingerprint();
    let client = self.manager.state::<crate::ManagedState>();
    client
        .event(machine_id, payload)
        .await
        .map_err(crate::Error::HyprAnalytics)?;
    Ok(())
}
```

**What is attached to every analytics event** — this is the complete enrichment logic:

```rust
// plugins/analytics/src/ext.rs
fn enrich_payload(
    manager: &M,
    payload: &mut hypr_analytics::AnalyticsPayload,
) {
    let app_version = env!("APP_VERSION");
    let app_identifier = manager.config().identifier.clone();
    let git_hash = manager.misc().get_git_hash();
    let bundle_id = manager.config().identifier.clone();

    payload.props.entry("app_version".into())
        .or_insert(app_version.into());
    payload.props.entry("app_identifier".into())
        .or_insert(app_identifier.into());
    payload.props.entry("git_hash".into())
        .or_insert(git_hash.into());
    payload.props.entry("bundle_id".into())
        .or_insert(bundle_id.into());
    payload.props.entry("$set".into()).or_insert_with(|| {
        serde_json::json!({ "app_version": app_version })
    });
}
```

**What is sent:**
- Event names (e.g. "session_started", "transcription_completed")
- App version, build identifier, git hash, bundle ID
- A device fingerprint (a hashed machine identifier via `hypr_host::fingerprint()`, not your name, email, or IP)
- When signed in: your user ID and email (for account-linked analytics)

**Where it goes:**
- [PostHog](https://posthog.com) — product analytics. [Privacy Policy](https://posthog.com/privacy)
- [Outlit](https://outlit.dev) — product analytics

The analytics client sends events to both services:

```rust
// crates/analytics/src/lib.rs
pub async fn event(
    &self,
    distinct_id: impl Into<String>,
    payload: AnalyticsPayload,
) -> Result<(), Error> {
    let distinct_id = distinct_id.into();

    if let Some(posthog) = &self.posthog {
        posthog
            .event(&distinct_id, &payload.event, &payload.props)
            .await?;
    }

    if let Some(outlit) = &self.outlit {
        outlit.event(&distinct_id, &payload).await;
    }

    Ok(())
}
```

**How to disable:** Go to Settings and turn off analytics. When disabled, no analytics events are sent — the `is_disabled()` check short-circuits the entire flow.

### Error Reporting (Sentry)

Char uses Sentry for crash reporting and error tracking in release builds. Here is the complete initialization:

```rust
// apps/desktop/src-tauri/src/lib.rs
let sentry_client = {
    let dsn = option_env!("SENTRY_DSN");

    if let Some(dsn) = dsn {
        let release = option_env!("APP_VERSION")
            .map(|v| format!("hyprnote-desktop@{}", v).into());

        let client = sentry::init((
            dsn,
            sentry::ClientOptions {
                release,
                traces_sample_rate: 1.0,
                auto_session_tracking: false,
                ..Default::default()
            },
        ));

        sentry::configure_scope(|scope| {
            scope.set_tag("service", "hyprnote-desktop");
            scope.set_user(Some(sentry::User {
                id: Some(hypr_host::fingerprint()),
                ..Default::default()
            }));
        });

        Some(client)
    } else {
        None
    }
};
```

**What is sent:**
- Error messages, stack traces, and crash dumps
- A device fingerprint (hashed machine ID via `hypr_host::fingerprint()`) — no name, email, or IP
- App version and platform information (`hyprnote-desktop@{version}`)
- The tag `service: "hyprnote-desktop"`
- `auto_session_tracking` is explicitly set to `false`

**Where it goes:**
- [Sentry](https://sentry.io) — error monitoring. [Privacy Policy](https://sentry.io/privacy/)

### Cloud Transcription (When Enabled)

When you use cloud-based transcription, your audio is sent to the selected provider for processing. **When using local transcription (Whisper or Argmax models), your audio never leaves your device.**

Here is how the listener actor connects to your configured STT provider — audio is streamed via the adapter matching your settings:

```rust
// plugins/listener/src/actors/listener/adapters.rs
pub(super) async fn spawn_rx_task(
    args: ListenerArgs,
    myself: ActorRef<ListenerMsg>,
) -> Result<(ChannelSender, JoinHandle<()>, oneshot::Sender<()>, String), ActorProcessingErr> {
    let adapter_kind = AdapterKind::from_url_and_languages(
        &args.base_url,
        &args.languages,
        Some(&args.model),
    );

    let result = match (adapter_kind, is_dual) {
        (AdapterKind::Argmax, false) =>
            spawn_rx_task_single_with_adapter::<ArgmaxAdapter>(args, myself).await,
        (AdapterKind::Soniox, false) =>
            spawn_rx_task_single_with_adapter::<SonioxAdapter>(args, myself).await,
        (AdapterKind::Deepgram, false) =>
            spawn_rx_task_single_with_adapter::<DeepgramAdapter>(args, myself).await,
        (AdapterKind::AssemblyAI, false) =>
            spawn_rx_task_single_with_adapter::<AssemblyAIAdapter>(args, myself).await,
        (AdapterKind::OpenAI, false) =>
            spawn_rx_task_single_with_adapter::<OpenAIAdapter>(args, myself).await,
        (AdapterKind::Gladia, false) =>
            spawn_rx_task_single_with_adapter::<GladiaAdapter>(args, myself).await,
        (AdapterKind::ElevenLabs, false) =>
            spawn_rx_task_single_with_adapter::<ElevenLabsAdapter>(args, myself).await,
        (AdapterKind::DashScope, false) =>
            spawn_rx_task_single_with_adapter::<DashScopeAdapter>(args, myself).await,
        (AdapterKind::Mistral, false) =>
            spawn_rx_task_single_with_adapter::<MistralAdapter>(args, myself).await,
        (AdapterKind::Fireworks, false) =>
            spawn_rx_task_single_with_adapter::<FireworksAdapter>(args, myself).await,
        // ...dual-channel variants also exist for each provider
    }?;

    Ok((result.0, result.1, result.2, adapter_kind.to_string()))
}
```

The `ListenerArgs` passed to the STT adapter contain the following — this is all the data sent to the provider along with your audio stream:

```rust
// plugins/listener/src/actors/session/supervisor.rs
ListenerArgs {
    app: state.ctx.app.clone(),
    languages: state.ctx.params.languages.clone(),
    onboarding: state.ctx.params.onboarding,
    model: state.ctx.params.model.clone(),
    base_url: state.ctx.params.base_url.clone(),
    api_key: state.ctx.params.api_key.clone(),
    keywords: state.ctx.params.keywords.clone(),
    mode, // MicOnly, SpeakerOnly, or MicAndSpeaker
    session_started_at: state.ctx.started_at_instant,
    session_started_at_unix: state.ctx.started_at_system,
    session_id: state.ctx.params.session_id.clone(),
}
```

**What is sent:**
- Your recorded audio (streamed in real-time or sent as a file for batch transcription)
- Configured language preferences and keywords
- The model name and API key for your provider

**Where it goes depends on your setup:**

- **Pro curated models:** Audio is proxied through `pro.hyprnote.com` (our server) and forwarded to a curated STT provider. The proxy does not store your audio.
- **BYOK (Bring Your Own Key):** Audio is sent directly from your device to the provider you selected.

**Supported cloud STT providers:**

| Provider | Privacy Policy |
|----------|---------------|
| Deepgram | [Privacy Policy](https://deepgram.com/privacy) |
| AssemblyAI | [Privacy Policy](https://www.assemblyai.com/legal/privacy-policy) |
| Soniox | [Privacy Policy](https://soniox.com/privacy) |
| Gladia | [Privacy Policy](https://www.gladia.io/privacy-policy) |
| OpenAI | [Privacy Policy](https://openai.com/policies/privacy-policy) |
| ElevenLabs | [Privacy Policy](https://elevenlabs.io/privacy-policy) |
| DashScope | [Privacy Policy](https://www.alibabacloud.com/help/en/legal/latest/Chinese-mainland-chinese-version-Chinese-mainland-chinese-version) |
| Mistral | [Privacy Policy](https://mistral.ai/terms/#privacy-policy) |
| Fireworks AI | [Privacy Policy](https://fireworks.ai/privacy-policy) |

### Cloud LLM (When Enabled)

When you use cloud-based AI features (summaries, enhanced notes, chat), your session content is sent to the selected LLM provider. **When using local LLMs (LM Studio or Ollama), everything stays on your device.**

Here is how the language model client is created — each provider connects directly to its own API:

```typescript
// apps/desktop/src/hooks/useLLMConnection.ts
const createLanguageModel = (conn: LLMConnectionInfo): LanguageModelV3 => {
  switch (conn.providerId) {
    case "hyprnote": {
      const provider = createOpenRouter({
        fetch: tracedFetch,
        baseURL: conn.baseUrl,
        apiKey: conn.apiKey,
      });
      return wrapWithThinkingMiddleware(provider.chat(conn.modelId));
    }
    case "anthropic": {
      const provider = createAnthropic({
        fetch: tauriFetch,
        apiKey: conn.apiKey,
        headers: {
          "anthropic-version": "2023-06-01",
          "anthropic-dangerous-direct-browser-access": "true",
        },
      });
      return wrapWithThinkingMiddleware(provider(conn.modelId));
    }
    case "google_generative_ai": {
      const provider = createGoogleGenerativeAI({
        fetch: tauriFetch,
        baseURL: conn.baseUrl,
        apiKey: conn.apiKey,
      });
      return wrapWithThinkingMiddleware(provider(conn.modelId));
    }
    case "openai": {
      const provider = createOpenAI({
        fetch: tauriFetch,
        baseURL: conn.baseUrl,
        apiKey: conn.apiKey,
      });
      return wrapWithThinkingMiddleware(provider(conn.modelId));
    }
    case "ollama": {
      const provider = createOpenAICompatible({
        fetch: ollamaFetch,
        name: conn.providerId,
        baseURL: conn.baseUrl,
      });
      return wrapWithThinkingMiddleware(
        provider.chatModel(conn.modelId),
      );
    }
  }
};
```

When auto-enhance runs after a session ends, the enhanced result is stored locally:

```typescript
// apps/desktop/src/hooks/autoEnhance/runner.ts
const handleEnhanceSuccess = useCallback(
  (text: string) => {
    const noteId = currentNoteIdRef.current;
    if (!text || !store || !noteId) return;
    const jsonContent = md2json(text);
    store.setPartialRow("enhanced_notes", noteId, {
      content: JSON.stringify(jsonContent),
    });
  },
  [store, sessionId, model, titleTask.start],
);
```

An analytics event is also fired when auto-enhance runs — it includes only the provider and model name, not the content:

```typescript
// apps/desktop/src/hooks/autoEnhance/runner.ts
void analyticsCommands.event({
  event: "note_enhanced",
  is_auto: true,
  llm_provider: llmConn?.providerId,
  llm_model: llmConn?.modelId,
});
```

**What is sent:**
- Transcript text, raw notes, and prompt templates
- The content you are asking the AI to process

**Where it goes depends on your setup:**

- **Pro curated models:** Requests are proxied through `pro.hyprnote.com` and forwarded to a curated LLM provider. Nothing is stored by our proxy.
- **BYOK providers:** Requests are sent directly to the provider you selected (OpenAI, Anthropic, Google, or Mistral).
- **Local LLMs:** Everything stays on your device. See [Local LLM Setup](/docs/faq/local-llm-setup).

### MCP Tools (Pro Only)

Pro users have access to MCP tools for web search and URL reading during AI-assisted note generation.

**What is sent:**
- Search queries (for web search)
- URLs (for content extraction)

**Where it goes:**
- Proxied through `pro.hyprnote.com`
- [Exa](https://exa.ai) — web search
- [Jina AI](https://jina.ai) — URL content reading

### Network Connectivity Check

Char periodically checks if your device is online. Here is the complete implementation:

```rust
// plugins/network/src/actor.rs
const CHECK_INTERVAL: Duration = Duration::from_secs(2);
const CHECK_URL: &str = "https://www.google.com/generate_204";
const REQUEST_TIMEOUT: Duration = Duration::from_secs(5);

async fn check_network() -> bool {
    let client = reqwest::Client::builder()
        .timeout(REQUEST_TIMEOUT)
        .build();
    let client = match client {
        Ok(c) => c,
        Err(_) => return false,
    };
    match client.head(CHECK_URL).send().await {
        Ok(response) => {
            response.status().is_success()
                || response.status().as_u16() == 204
        }
        Err(_) => false,
    }
}
```

**What happens:**
- A HEAD request to `https://www.google.com/generate_204` every 2 seconds
- No user data, cookies, or identifiers are included — it is a bare HEAD request
- The response is only used to determine if the device is online (boolean)

### Model Downloads

When you download a local STT model, Char fetches the model file from a hosting server.

These are the supported local models:

```rust
// plugins/local-stt/src/model.rs
pub static SUPPORTED_MODELS: [SupportedSttModel; 10] = [
    SupportedSttModel::Whisper(WhisperModel::QuantizedTiny),
    SupportedSttModel::Whisper(WhisperModel::QuantizedTinyEn),
    SupportedSttModel::Whisper(WhisperModel::QuantizedBase),
    SupportedSttModel::Whisper(WhisperModel::QuantizedBaseEn),
    SupportedSttModel::Whisper(WhisperModel::QuantizedSmall),
    SupportedSttModel::Whisper(WhisperModel::QuantizedSmallEn),
    SupportedSttModel::Whisper(WhisperModel::QuantizedLargeTurbo),
    SupportedSttModel::Am(AmModel::ParakeetV2),
    SupportedSttModel::Am(AmModel::ParakeetV3),
    SupportedSttModel::Am(AmModel::WhisperLargeV3),
];
```

**What happens:**
- Standard HTTP download requests to S3 (Whisper models) or Argmax hosting servers
- No user data is sent
- Downloaded models are verified with checksums before use

### App Updates

Char checks for updates using the Tauri updater system.

**What is sent:**
- Your current app version and platform

**Where it goes:**
- [CrabNebula](https://crabnebula.dev) — release hosting and update distribution

### Authentication (When Signed In)

When you sign in for Pro or cloud features, Char authenticates via Supabase.

**What is stored locally:**
- Auth session tokens in the local Tauri store (`store.json`)
- Account info: user ID, email, full name, avatar URL

**What is sent:**
- Authentication requests to Supabase during sign-in
- Auth tokens to `pro.hyprnote.com` when using Pro features

### Cloud Database Sync (Optional)

Char supports optional cloud database sync.

**When enabled:** Your session data can be synced to a remote database. This is only active if you explicitly configure a cloud database connection.

**When not configured:** All data stays in the local SQLite database.

## What AI Models Does Char Use?

Char uses two types of AI models: speech-to-text (STT) for transcription and large language models (LLMs) for generating summaries and notes.

### Speech-to-Text Models

**Local models** run entirely on your device:
- Whisper models (QuantizedTiny, Base, Small, LargeTurbo) — downloaded as GGUF files
- Argmax models (ParakeetV2, ParakeetV3, WhisperLargeV3) — downloaded as tarballs

For local model details and download instructions, see [Local Models](/docs/developers/local-models).

**Cloud models** require sending audio to a provider. See the cloud transcription section above.

### Large Language Models

**Pro Curated Models** — Subscribe to Pro for curated cloud AI models that work out of the box.

**BYOK (Bring Your Own Key)** — Enter your own API key for OpenAI, Anthropic, Google, or Mistral.

**Local Models** — Run models locally using [LM Studio](https://lmstudio.ai) or [Ollama](https://ollama.com). See [Local LLM Setup](/docs/faq/local-llm-setup).

Recommended local models: **Gemma** (Google, good balance of quality and performance) and **Qwen** (Alibaba, strong multilingual support).

## Does Char Train AI Models on My Data?

No. Char does not use your recordings, transcripts, or notes to train AI models. When using cloud providers, your data is processed according to their respective privacy policies, but Char itself does not collect or use your data for training.

## What Char Does NOT Do

- Does **not** train AI models on your data
- Does **not** sell your data to third parties
- Does **not** collect your audio, transcripts, or notes for any purpose other than the features you explicitly use
- Does **not** send your meeting content in analytics — analytics only includes event names and app metadata

## Can I Use Char Completely Offline?

Yes, with local models. You can record audio, transcribe with a local Whisper or Argmax model, and generate summaries with a local LLM (LM Studio or Ollama) — all without an internet connection.

The only background network requests in a fully local setup are the connectivity check (HEAD to google.com) and update checks to CrabNebula.

## How to Maximize Privacy

1. Use a local STT model for transcription — see [Local Models](/docs/developers/local-models)
2. Use a local LLM (LM Studio or Ollama) for AI features — see [Local LLM Setup](/docs/faq/local-llm-setup)
3. Disable analytics in Settings
4. Do not sign in or enable any cloud features
5. Enable full-disk encryption on your OS (FileVault, LUKS, BitLocker)

## Open Source

Char is open source. You can verify everything documented here by reading the code:

- [Desktop app (Tauri backend)](https://github.com/fastrepl/char/tree/main/apps/desktop/src-tauri)
- [Pro cloud server](https://github.com/fastrepl/char/tree/main/apps/pro)
- [Analytics plugin](https://github.com/fastrepl/char/tree/main/plugins/analytics)
- [Analytics crate](https://github.com/fastrepl/char/tree/main/crates/analytics)
- [Listener plugin](https://github.com/fastrepl/char/tree/main/plugins/listener)
- [Local STT plugin](https://github.com/fastrepl/char/tree/main/plugins/local-stt)
- [Database plugin](https://github.com/fastrepl/char/tree/main/plugins/db2)
- [Network plugin](https://github.com/fastrepl/char/tree/main/plugins/network)
